{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOmvJ5+cQhS9bxjc4U5BGQt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hmerkle/deepRL/blob/main/DRL23_HW04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Excercise 4 -> not working\n",
        "\n",
        "datasets get empty and have not found the solution to this problem yet"
      ],
      "metadata": {
        "id": "BUdOzcCMDFhZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review: Group 38"
      ],
      "metadata": {
        "id": "vehybpKfiRtX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pip install and imports"
      ],
      "metadata": {
        "id": "1eYbnY0HFugX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"gymnasium[accept-rom-license, atari]\""
      ],
      "metadata": {
        "id": "tMdXfYgSE1SY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b8ef79e-11b7-4fde-d5d1-a40dccaa9cc2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gymnasium[accept-rom-license,atari]\n",
            "  Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (1.22.4)\n",
            "Collecting jax-jumpy>=1.0.0 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (4.6.3)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (8.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (4.65.0)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari]) (5.12.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (3.4)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=5bf95d5aee97e7fb2559eb570e599931ca42d408a83834d18ca2953a5764c3b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: farama-notifications, jax-jumpy, ale-py, gymnasium, AutoROM.accept-rom-license, autorom, shimmy\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.4.2 farama-notifications-0.0.4 gymnasium-0.28.1 jax-jumpy-1.0.0 shimmy-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade ipykernel"
      ],
      "metadata": {
        "id": "g-e4yY6MFek-"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "7Fo9NsQsEVFs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# code"
      ],
      "metadata": {
        "id": "Yi7mJZvHFyg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperienceReplayBuffer:\n",
        "    \"\"\"\n",
        "    max_size: maximum size of the replay buffer\n",
        "    env_name: name of the environment\n",
        "    parallel_game_unrolls: number of parallel games to play ( we run multiple independent copies of same env in parallel with gym.vactor.VectorEnv)\n",
        "    observation_preprocessing_function: function that preprocesses the observation\n",
        "    \"\"\"\n",
        "    def __init__(self, max_size:int, env_name: str, parallel_game_unrolls: int, observation_preprocessing_function: callable, unroll_steps: int):\n",
        "        self.max_size = max_size\n",
        "        self.env_name = env_name\n",
        "        self.parallel_game_unrolls = parallel_game_unrolls\n",
        "        self.unroll_steps = unroll_steps\n",
        "        self.observation_preprocessing_function = observation_preprocessing_function\n",
        "        self.envs = gym.vector.make(env_name, self.parallel_game_unrolls)\n",
        "        self.num_possible_actions = self.envs.single_action_space.n\n",
        "        self.current_states, _ = self.envs.reset()\n",
        "        self.data = [] # fill this up step by step\n",
        "\n",
        "    # def add_experience(self, state, action, reward, next_state, terminated):\n",
        "    #     self.buffer.append((state, action, reward, next_state, terminated))\n",
        "\n",
        "    def fill_with_samples(self, dqn_network, epsilon: float):\n",
        "        states_list = [] # length of this list is unroll_steps\n",
        "        actions_list = []\n",
        "        rewards_list = []\n",
        "        subsequent_states_list = []\n",
        "        terminateds_list = [] # each timestep is a batch\n",
        "\n",
        "        # in each of these steps we take 1 step in each of all our parallel_game_unrolls environments\n",
        "        # adds new samples into ERP\n",
        "        for i in range(self.unroll_steps):\n",
        "\n",
        "            actions = self.sample_epsilon_greedy_action(dqn_network, epsilon)\n",
        "            # print(\"actions: -> fill smaples with unroll steps\", actions.shape)\n",
        "            next_states, rewards, terminateds, _, _ = self.envs.step(actions)\n",
        "\n",
        "            # put state, action, reward, next observation into erp\n",
        "            states_list.append(self.current_states)\n",
        "            actions_list.append(actions)\n",
        "            rewards_list.append(rewards)\n",
        "            subsequent_states_list.append(next_states)\n",
        "            terminateds_list.append(terminateds)\n",
        "\n",
        "            self.current_states = next_states\n",
        "\n",
        "        # create a generator that yields a sample for each of the timesteps\n",
        "        def data_generator():\n",
        "\n",
        "            # walk through all timesteps created and for each timestep walk through all respective game to create 1 samle for each of these\n",
        "            for states_batch, actions_batch, rewards_batch, subsequent_states_batch, terminateds_batch in zip(states_list, actions_list, rewards_list, subsequent_states_list, terminateds_list):\n",
        "              # print(\"shapes batches: s, a, r, ss, t\", states_batch.shape, actions_batch.shape, rewards_batch.shape, subsequent_states_batch.shape, terminateds_batch.shape)\n",
        "              for game_idx in range(self.parallel_game_unrolls):\n",
        "                state = states_batch[game_idx, :, :, :]\n",
        "                action = actions_batch[game_idx]\n",
        "                reward = rewards_batch[game_idx]\n",
        "                subsequent_state = subsequent_states_batch[game_idx, :, :, :]\n",
        "                terminated = terminateds_batch[game_idx]\n",
        "                # print(\"shapes: s, a, r, ss, t \", state.shape, action.shape, reward.shape, subsequent_state.shape, terminated.shape)\n",
        "                yield (state, action, reward, subsequent_state, terminated)\n",
        "\n",
        "        # print(\"self.envs.action_space.shape: \", self.envs.action_space.shape)\n",
        "        dataset_tensor_specs = (tf.TensorSpec(shape=(210, 160, 3), dtype=tf.uint8),\n",
        "                                tf.TensorSpec(shape=(), dtype=tf.int32),#self.envs.action_space.shape\n",
        "                                tf.TensorSpec(shape = (), dtype=tf.float32),\n",
        "                                tf.TensorSpec(shape=(210, 160, 3), dtype=tf.uint8),\n",
        "                                tf.TensorSpec(shape = (), dtype=tf.bool))\n",
        "\n",
        "        # create for each step, we create sperate dataset\n",
        "        new_samples_dataset = tf.data.Dataset.from_generator(data_generator, output_signature=dataset_tensor_specs)\n",
        "        # print(\"self.observation_preprocessing_function(state): \", states_list[0].shape, (self.observation_preprocessing_function(states_list[0])).shape)\n",
        "        # Apply preprocessing first\n",
        "        new_samples_dataset = new_samples_dataset.map(lambda state, action, reward, subsequent_state, terminated:(self.observation_preprocessing_function(state), action, reward, self.observation_preprocessing_function(subsequent_state), terminated)) #TODO\n",
        "        # shuffle on each iteration\n",
        "        new_samples_dataset = new_samples_dataset.cache()\n",
        "        new_samples_dataset = new_samples_dataset.shuffle(buffer_size=self.parallel_game_unrolls*self.unroll_steps, reshuffle_each_iteration=True)\n",
        "        #print(\"new_sample_dataset: \", new_sample_dataset.shape)\n",
        "\n",
        "        # make sure cache is applied!\n",
        "        count = 0\n",
        "        for elem in new_samples_dataset:\n",
        "          continue\n",
        "        # print(\"new_samples_dataset secured\")\n",
        "\n",
        "        self.data.append(new_samples_dataset)\n",
        "        #print(self.data, len(self.data))\n",
        "        datapoints_in_data = len(self.data) * self.parallel_game_unrolls * self.unroll_steps # last two calculate how many s, a, s', t tuples in one element of datalist\n",
        "        if datapoints_in_data > self.max_size:\n",
        "          self.data.pop(0)\n",
        "\n",
        "\n",
        "    def create_dataset(self):\n",
        "        # creates a tf.data.Dataset object from the ERP\n",
        "        erp_dataset = tf.data.Dataset.sample_from_datasets(self.data, weights = [1/float(len(self.data)) for _ in self.data], stop_on_empty_dataset = False) # choose from datasets? #TODO\n",
        "        return erp_dataset\n",
        "\n",
        "    def sample_epsilon_greedy_action(self, dqn_network, epsilon: float):\n",
        "\n",
        "      # samples actions from dqn_network using epsilon greedy fashion\n",
        "      observations = self.observation_preprocessing_function(self.current_states)\n",
        "      # print(\"self.current_states: (6,(210, 160, 3))\", (self.current_states).shape)\n",
        "      q_values = dqn_network(observations) # tensor of type tf.float32, shape (parallel_game_unrolls, num_actions)\n",
        "      # print(\"q_values: \", q_values.shape)\n",
        "      greedy_actions = tf.argmax(q_values, axis=1) # tensor of type tf.int64, shape (parallel_game_unrolls, )\n",
        "      # print(\"greedy_actions: \", greedy_actions.shape)\n",
        "      random_actions = tf.random.uniform(shape=(self.parallel_game_unrolls,), minval=0, maxval=self.num_possible_actions, dtype=tf.int64) # tensor of type tf.int64, shape (parallel_game_unrolls, )\n",
        "      # print(\"self.random_actions:\", random_actions.shape)\n",
        "      # sample random actions with probability epsilon\n",
        "      # prob 1-epsilon to take greedy action -> (p > epsilon) is true\n",
        "      # prob epsilon to take random action\n",
        "      greedy_actions_mask = tf.random.uniform(shape=(self.parallel_game_unrolls,), minval=0, maxval=1, dtype=tf.float32) > epsilon # tensor of type tf.bool, shape (parallel_game_unrolls, )\n",
        "      actions = tf.where(greedy_actions_mask, greedy_actions, random_actions).numpy() # tensor of type tf.int64, shape (parallel_game_unrolls, )\n",
        "      return actions"
      ],
      "metadata": {
        "id": "eAFivu92ZPjg"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def observation_preprocessing_function(observation):\n",
        "    # (210, 160, 3) to (84, 84, 3)\n",
        "    #converts an observation to a tensor of type tf.float32, size(84,84)\n",
        "    observation = tf.image.resize(observation, size=(84,84))#, resize=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    observation = tf.cast(observation, dtype=tf.float32) /128.0 - 1.0 # values between -1 and 1\n",
        "\n",
        "    return observation"
      ],
      "metadata": {
        "id": "TouJpOSjvZRb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dqn_network(num_actions: int):\n",
        "    #create an input for tf functional model api\n",
        "    input_layer = tf.keras.Input(shape=(84, 84, 3), dtype= tf.float32)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu')(input_layer)\n",
        "    x = tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu', padding='same')(x) + x #residual connection\n",
        "    x = tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu', padding='same')(x) + x #residual connection\n",
        "    x = tf.keras.layers.MaxPool2D(pool_size=2)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu')(x)\n",
        "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')(x) + x #residual connection\n",
        "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')(x) + x #residual connection\n",
        "    x = tf.keras.layers.MaxPool2D(pool_size=2)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu')(x)\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x) + x #residual connection\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x) + x #residual connection\n",
        "    x = tf.keras.layers.MaxPool2D(pool_size=2)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu')(x)\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x) + x #residual connection\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x) + x #residual connection\n",
        "    x = tf.keras.layers.GlobalAvgPool2D()(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(units=64, activation='relu')(x) + x #residual connection\n",
        "    x = tf.keras.layers.Dense(units=num_actions, activation='linear')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=x)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "TLSwzH9jvapP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dqn(train_network, target_network, dataset, optimizer, gamma: float, num_training_steps:int, batch_size: int):\n",
        "\n",
        "    dataset = dataset.batch(batch_size).prefetch(4)\n",
        "\n",
        "    @tf.function\n",
        "    def training_step(q_targets, observations, actions):\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_predictions_all_actions = train_network(observations) #shape of q_predictions is (batch_size, num_actions)\n",
        "            q_predictions = tf.gather(q_predictions_all_actions, actions, batch_dims=1) # find q-values at right indices from q_predictions_all_actions\n",
        "            loss = tf.reduce_mean(tf.square(q_predictions - q_targets))\n",
        "        gradients = tape.gradient(loss, train_network.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, train_network.trainable_variables))\n",
        "        return loss.numpy()\n",
        "\n",
        "    losses = []\n",
        "    q_values = []\n",
        "    #some preparations\n",
        "    for i, state_transition in enumerate(dataset):\n",
        "        state, action, reward, subsequent_state, terminated = state_transition\n",
        "        #calculate q_targets\n",
        "        q_vals =  target_network(subsequent_state) #TODO: check if this is correct\n",
        "        q_values.append(q_vals.numpy())\n",
        "        max_q_values = tf.reduce_max(q_vals, axis=1)\n",
        "        use_subsequent_state = tf.where(terminated, tf.zeros_like(max_q_values, dtype=tf.float32), tf.ones_like(max_q_values, dtype=tf.float32))\n",
        "        # calculate q_targets\n",
        "        q_target = reward + (gamma*max_q_values*use_subsequent_state)\n",
        "        #train on data\n",
        "        loss = training_step(q_target, observations=state, actions=action).numpy()\n",
        "        losses.append(loss)\n",
        "        if i >= num_training_steps:\n",
        "            break\n",
        "\n",
        "    return np.mean(losses), np.mean(q_values)"
      ],
      "metadata": {
        "id": "JoZpoCoqHKMZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_dqn_network(test_dqn_network, env_name: str, num_parallel_game_tests:int, gamma: float, preprocessing_function: callable, test_epsilon: float=0.05):\n",
        "    envs = gym.vector.make(env_name, num_envs=num_parallel_game_tests)\n",
        "    num_possible_actions = envs.single_action_space.n\n",
        "    states,_ = envs.reset()\n",
        "\n",
        "    #episodes finished is a numpy vector of shape (num_parallel_tests), filled with Booleans, starting with all False\n",
        "    episodes_finished = np.zeros(num_parallel_game_tests, dtype=bool)\n",
        "    returns = np.zeros(num_parallel_game_tests)\n",
        "\n",
        "    done=False\n",
        "    timestep = 0\n",
        "    while not done:\n",
        "        states = preprocessing_function(states)\n",
        "        q_values = test_dqn_network(states)\n",
        "        greedy_actions = tf.argmax(q_values, axis=1) # tensor of type tf.int64, shape (num_parallel_game_tests,)\n",
        "        random_actions = tf.random.uniform(shape=(num_parallel_game_tests,), minval=0, maxval=num_possible_actions, dtype=tf.int64)\n",
        "        epsilon_sampling = tf.random.uniform(shape=(num_parallel_game_tests,), minval=0, maxval=1, dtype=tf.float32) > test_epsilon\n",
        "        actions = tf.where(epsilon_sampling, greedy_actions, random_actions).numpy() # type numpy.ndarray, shape (num_parallel_game_tests,)\n",
        "        states, rewards, terminateds, _, _ = envs.step(actions)\n",
        "\n",
        "        #compute pointwise or between episodes_finished and terminateds\n",
        "        episodes_finished = np.logical_or(episodes_finished, terminateds) # if an episode is finished, it stays finished\n",
        "        returns += ((gamma**timestep)*rewards)*(np.logical_not(episodes_finished).astype(np.float32)) # only add rewards if episode is not finished -> multiply with 0 if episode is finished\n",
        "        timestep += 1\n",
        "        #done if all episodes are finished\n",
        "        done = np.all(episodes_finished)\n",
        "        test_steps += 1\n",
        "        if test_steps%100 == 0:\n",
        "            print(\"Test steps: \", test_steps, np.sum(episodes_finished)/num_parallel_game_tests, terminateds.shape, episodes_finished.shape)\n",
        "\n",
        "    return np.mean(returns)"
      ],
      "metadata": {
        "id": "wu6_eWumvf5P"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Take a fraction of the source network weights and copy weights to target network weights\n",
        "# def polyak_averaging_weigths(source_network, target_network, polyak_averaging_factor: float):\n",
        "#     source_network_weights = source_network.get_weights()\n",
        "#     target_network_weights = target_network.get_weights()\n",
        "#     averaged_weights = []\n",
        "\n",
        "#     for source_network_weight, target_network_weight in zip(source_network_weights, target_network_weights):\n",
        "#         fraction_kept_weights = polyak_averaging_factor*target_network_weight # weight that are kept from previous iteration\n",
        "#         fraction_updated_weights = (1 - polyak_averaging_factor) * source_network_weight\n",
        "#         average_weight = fraction_kept_weights + fraction_updated_weights\n",
        "#         averaged_weights.append(average_weight)\n",
        "#     target_network.set_weights(averaged_weights)"
      ],
      "metadata": {
        "id": "CnTwig50vjro"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a fraction of the source network weights and copy weights to target network weights\n",
        "def polyak_averaging_weigths(source_network, target_network, polyak_averaging_factor: float):\n",
        "    source_network_weights = source_network.get_weights()\n",
        "    target_network_weights = target_network.get_weights()\n",
        "    averaged_weights = []\n",
        "\n",
        "    for source_network_weight, target_network_weight in zip(source_network_weights, target_network_weights):\n",
        "        fraction_kept_weights = polyak_averaging_factor*source_network_weight # weight that are kept from previous iteration\n",
        "        fraction_updated_weights = (1 - polyak_averaging_factor) * target_network_weight\n",
        "        average_weight = fraction_kept_weights + fraction_updated_weights\n",
        "        averaged_weights.append(average_weight)\n",
        "    target_network.set_weights(averaged_weights)"
      ],
      "metadata": {
        "id": "ajjM4w7-MkJ7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualise_results(result_df, step):\n",
        "    # create a figure with 3 subplots\n",
        "    fig, axis = plt.subplots(3, 1)\n",
        "    # include row idxs explicitely in results_df\n",
        "    result_df['step'] = result_df.index\n",
        "    # plot average returns\n",
        "    sns.lineplot(x = \"step\", y = \"average_returns\", data = result_df, ax = axis[0])\n",
        "    # plot average losses\n",
        "    sns.lineplot(x = \"step\", y = \"average_losses\", data = result_df, ax = axis[1])\n",
        "    # plot average q-values\n",
        "    sns.lineplot(x = \"step\", y = \"average_q_values\", data = result_df, ax = axis[2])\n",
        "    # # save the figure\n",
        "    # plt.savefig(\"average_q_values.png\")\n",
        "    # # create a timestring from the timestamp\n",
        "    # timestring = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    # # save the figure\n",
        "    # plt.savefig(f'./results/{timestring}_results_step{step}.png')\n",
        "    # plt.close(fig)"
      ],
      "metadata": {
        "id": "J6grNi7Svmqp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dqn():\n",
        "    ENVIRONMENT_NAME = 'ALE/Breakout-v5'\n",
        "    NUM_ACTIONS = gym.make(ENVIRONMENT_NAME).action_space.n\n",
        "    ERP_SIZE= 100#100000 # experience replay buffer size\n",
        "    PARALLEL_GAME_UNROLLS = 6#24#128 # number of parallel games to play\n",
        "    UNROLL_STEPS = 24 # number of steps to unroll each game\n",
        "    EPSILON = 0.2\n",
        "    GAMMA = 0.995\n",
        "    NUM_TRAINING_STEPS_PER_ITERATION = 16\n",
        "    TRAIN_BATCH_SIZE = 128#512\n",
        "    NUM_TRAINING_ITERS = 3#50000\n",
        "    TEST_EVERY_N_STEPS = 50\n",
        "    TEST_NUM_PARALLEL_ENVS = 6#24#128\n",
        "    PREFILL_STEPS = 20#24#100\n",
        "    POLYAK_AVERAGING_FACTOR = 0.99\n",
        "\n",
        "    # container containing all  s, a, r, s', t transitions\n",
        "    # With deep Q-networks, we often utilize this technique called experience replay during training. With experience replay, we store\n",
        "    # the agent's experiences at each time step in a data set called the replay memory\n",
        "    erp = ExperienceReplayBuffer(max_size=ERP_SIZE,\n",
        "                                env_name=ENVIRONMENT_NAME,\n",
        "                                parallel_game_unrolls=PARALLEL_GAME_UNROLLS,\n",
        "                                observation_preprocessing_function=observation_preprocessing_function,\n",
        "                                unroll_steps=UNROLL_STEPS)\n",
        "\n",
        "    #input is some image from game and outputs atari output\n",
        "    # dqn that is trained\n",
        "    dqn_agent = create_dqn_network(num_actions=NUM_ACTIONS)\n",
        "    #this is the target network, used to calculate the q_estimation targets\n",
        "    target_network = create_dqn_network(num_actions=NUM_ACTIONS)\n",
        "\n",
        "    #dqn_agent(tf.random.uniform(shape=(1, 84, 84, 3)))\n",
        "    #dqn_agent.summary()\n",
        "\n",
        "    #copy over the weights from the dqn_agent to the target_network via polyak averaging with factor 0.8\n",
        "    polyak_averaging_weigths(source_network=dqn_agent, target_network=target_network, polyak_averaging_factor=0.0)#0.8)\n",
        "\n",
        "    dqn_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "    #prefill the replay buffer -> less  bias\n",
        "    prefill_exploration_epsilon = 1.0\n",
        "    for prefill_step in range(PREFILL_STEPS):\n",
        "        erp.fill_with_samples(dqn_agent, prefill_exploration_epsilon)\n",
        "\n",
        "    #test the agent\n",
        "    return_tracker = []\n",
        "    dqn_prediction_error = []\n",
        "    average_q_values = []\n",
        "\n",
        "    for step in range(NUM_TRAINING_ITERS):\n",
        "        print(\"Training iteration: \", step)\n",
        "        #step 1: put some s, a, r, s' transitions into the replay buffer\n",
        "        erp.fill_with_samples(dqn_agent, EPSILON)\n",
        "        dataset = erp.create_dataset()\n",
        "\n",
        "        #step 2: train on some samples from the replay buffer\n",
        "        average_loss, average_q_value = train_dqn(dqn_agent, target_network, dataset, optimizer=dqn_optimizer, gamma=GAMMA, num_training_steps=NUM_TRAINING_STEPS_PER_ITERATION, batch_size=TRAIN_BATCH_SIZE)\n",
        "        #update the target network via polyak averaging\n",
        "        erp.polyak_averaging_weights(source_network=dqn_agent, target_network=target_network, polyak_averaging_factor=POLYAK_AVERAGING_FACTOR)\n",
        "\n",
        "        #step 3: test the agent\n",
        "        if step % TEST_EVERY_N_STEPS == 0:  #test every N steps\n",
        "            average_returns = test_dqn_network(dqn_agent, ENVIRONMENT_NAME, num_parallel_tests=TEST_NUM_PARALLEL_ENVS, gamma=GAMMA )\n",
        "            return_tracker.append(average_returns)\n",
        "            dqn_prediction_error.append(average_loss)\n",
        "            average_q_values.append(average_q_value)\n",
        "            #print average loss, average returns, average q_values\n",
        "            print(f'average return: {average_returns}, TESTING: average loss: {average_loss}, average q_value-estimate: {average_q_value}')\n",
        "\n",
        "            #put all the result lists into a dataframe (by transforming them into a dict first)\n",
        "            results_dict = {'average_return': return_tracker, 'average_loss': dqn_prediction_error, 'average_q_values': average_q_values }\n",
        "            results_df = pd.Dataframe(results_dict)\n",
        "\n",
        "            #visualise the results with sns\n",
        "            # create 3 subplots\n",
        "            visualise_results(results_df, step)"
      ],
      "metadata": {
        "id": "ZNhl0w1MBm7M"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    dqn()"
      ],
      "metadata": {
        "id": "a7uwilXTLrYI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1781007d-47c0-46ca-e22a-7de781f77e5a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new_samples_dataset secured\n",
            "[<_ShuffleDataset element_spec=(TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.bool, name=None))>] 1\n",
            "new_samples_dataset secured\n",
            "[<_ShuffleDataset element_spec=(TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.bool, name=None))>] 1\n",
            "new_samples_dataset secured\n",
            "[<_ShuffleDataset element_spec=(TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.bool, name=None))>] 1\n",
            "new_samples_dataset secured\n",
            "[<_ShuffleDataset element_spec=(TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.bool, name=None))>] 1\n",
            "new_samples_dataset secured\n",
            "[<_ShuffleDataset element_spec=(TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.bool, name=None))>] 1\n",
            "new_samples_dataset secured\n",
            "[<_ShuffleDataset element_spec=(TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.bool, name=None))>] 1\n",
            "new_samples_dataset secured\n",
            "[<_ShuffleDataset element_spec=(TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.bool, name=None))>] 1\n",
            "new_samples_dataset secured\n",
            "[<_ShuffleDataset element_spec=(TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.bool, name=None))>] 1\n",
            "new_samples_dataset secured\n",
            "[<_ShuffleDataset element_spec=(TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.bool, name=None))>] 1\n",
            "new_samples_dataset secured\n",
            "[<_ShuffleDataset element_spec=(TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.bool, name=None))>] 1\n",
            "new_samples_dataset secured\n",
            "[<_ShuffleDataset element_spec=(TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.bool, name=None))>] 1\n",
            "new_samples_dataset secured\n",
            "[<_ShuffleDataset element_spec=(TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.bool, name=None))>] 1\n",
            "new_samples_dataset secured\n",
            "[<_ShuffleDataset element_spec=(TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.bool, name=None))>] 1\n",
            "new_samples_dataset secured\n",
            "[<_ShuffleDataset element_spec=(TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.bool, name=None))>] 1\n",
            "new_samples_dataset secured\n",
            "[<_ShuffleDataset element_spec=(TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.bool, name=None))>] 1\n",
            "new_samples_dataset secured\n",
            "[<_ShuffleDataset element_spec=(TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.bool, name=None))>] 1\n",
            "new_samples_dataset secured\n",
            "[<_ShuffleDataset element_spec=(TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.bool, name=None))>] 1\n",
            "new_samples_dataset secured\n",
            "[<_ShuffleDataset element_spec=(TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.bool, name=None))>] 1\n",
            "new_samples_dataset secured\n",
            "[<_ShuffleDataset element_spec=(TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.bool, name=None))>] 1\n",
            "new_samples_dataset secured\n",
            "[<_ShuffleDataset element_spec=(TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.bool, name=None))>] 1\n",
            "Training iteration:  0\n",
            "new_samples_dataset secured\n",
            "[<_ShuffleDataset element_spec=(TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(84, 84, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.bool, name=None))>] 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-bc5b70255259>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-55-430ebc23f487>\u001b[0m in \u001b[0;36mdqn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m#step 1: put some s, a, r, s' transitions into the replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0merp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_with_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdqn_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPSILON\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m#step 2: train on some samples from the replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-fe403c525e06>\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# creates a tf.data.Dataset object from the ERP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0merp_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_from_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_on_empty_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# choose from datasets? #TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0merp_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36msample_from_datasets\u001b[0;34m(datasets, weights, seed, stop_on_empty_dataset, rerandomize_each_iteration)\u001b[0m\n\u001b[1;32m   3569\u001b[0m     \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3570\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msample_from_datasets_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3571\u001b[0;31m     return sample_from_datasets_op._sample_from_datasets(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3572\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3573\u001b[0m         \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/sample_from_datasets_op.py\u001b[0m in \u001b[0;36m_sample_from_datasets\u001b[0;34m(datasets, weights, seed, stop_on_empty_dataset, rerandomize_each_iteration)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid `datasets`. `datasets` should not be empty.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid `datasets`. `datasets` should not be empty."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def dqn():\n",
        "#     ENVIRONMENT_NAME = 'ALE/Breakout-v5'\n",
        "#     NUM_ACTIONS = gym.make(ENVIRONMENT_NAME).action_space.n\n",
        "#     ERP_SIZE= 1000#100000 # experience replay buffer size\n",
        "#     PARALLEL_GAME_UNROLLS = 24#128 # number of parallel games to play\n",
        "#     UNROLL_STEPS = 4 # number of steps to unroll each game\n",
        "#     EPSILON = 0.2\n",
        "#     GAMMA = 0.995\n",
        "#     NUM_TRAINING_STEPS_PER_ITERATION = 16\n",
        "#     TRAIN_BATCH_SIZE = 128#512\n",
        "#     NUM_TRAINING_ITERS = 3#50000\n",
        "#     TEST_EVERY_N_STEPS = 50\n",
        "#     TEST_NUM_PARALLEL_ENVS = 24#128\n",
        "#     PREFILL_STEPS = 24#100\n",
        "#     POLYAK_AVERAGING_FACTOR = 0.99\n",
        "\n",
        "#     # container containing all  s, a, r, s', t transitions\n",
        "#     erp = ExperienceReplayBuffer(max_size=ERP_SIZE,\n",
        "#                                 env_name=ENVIRONMENT_NAME,\n",
        "#                                 parallel_game_unrolls=PARALLEL_GAME_UNROLLS,\n",
        "#                                 observation_preprocessing_function=observation_preprocessing_function,\n",
        "#                                 unroll_steps=UNROLL_STEPS)\n",
        "\n",
        "#     #input is some image from game and outputs atari output\n",
        "#     # dqn that is trained\n",
        "#     dqn_agent = create_dqn_network(num_actions=NUM_ACTIONS)\n",
        "#     #dqn_agent.summary()\n",
        "#     dqn_agent(tf.random.uniform(shape=(1, 84, 84, 3)))\n",
        "\n",
        "#     #this is the target network, used to calculate the q_estimation targets\n",
        "#     target_network = create_dqn_network(num_actions=NUM_ACTIONS)\n",
        "#     #copy over the weights from the dqn_agent to the target_network via polyak averaging with factor 0.8\n",
        "#     polyak_averaging_weigths(source_network=dqn_agent, target_network=target_network, polyak_averaging_factor=0.0)#0.8)\n",
        "\n",
        "#     dqn_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "#     #prefill the replay buffer -> less  bias\n",
        "#     prefill_exploration_epsilon = 1.0\n",
        "#     for prefill_step in range(PREFILL_STEPS):\n",
        "#         erp.fill_with_samples(dqn_agent, prefill_exploration_epsilon)\n",
        "\n",
        "#     #test the agent\n",
        "#     return_tracker = []\n",
        "#     dqn_prediction_error = []\n",
        "#     average_q_values = []\n",
        "\n",
        "#     for step in range(NUM_TRAINING_ITERS):\n",
        "#         print(\"Training iteration: \", step)\n",
        "#         #step 1: put some s, a, r, s' transitions into the replay buffer\n",
        "#         erp.fill_with_samples(dqn_agent, EPSILON)\n",
        "#         dataset = erp.create_dataset()\n",
        "\n",
        "#         #step 2: train on some samples from the replay buffer\n",
        "#         average_loss, average_q_value = train_dqn(dqn_agent, target_network, dataset, optimizer=dqn_optimizer, gamma=GAMMA, num_training_steps=NUM_TRAINING_STEPS_PER_ITERATION, batch_size=TRAIN_BATCH_SIZE)\n",
        "#         #update the target network via polyak averaging\n",
        "#         erp.polyak_averaging_weights(source_network=dqn_agent, target_network=target_network, polyak_averaging_factor=POLYAK_AVERAGING_FACTOR)\n",
        "\n",
        "#         #step 3: test the agent\n",
        "#         if step % TEST_EVERY_N_STEPS == 0:  #test every N steps\n",
        "#             average_returns = test_dqn_network(dqn_agent, ENVIRONMENT_NAME, num_parallel_tests=TEST_NUM_PARALLEL_ENVS, gamma=GAMMA )\n",
        "#             return_tracker.append(average_returns)\n",
        "#             dqn_prediction_error.append(average_loss)\n",
        "#             average_q_values.append(average_q_value)\n",
        "#             #print average loss, average returns, average q_values\n",
        "#             print(f'average return: {average_returns}, TESTING: average loss: {average_loss}, average q_value-estimate: {average_q_value}')\n",
        "\n",
        "#             #put all the result lists into a dataframe (by transforming them into a dict first)\n",
        "#             results_dict = {'average_return': return_tracker, 'average_loss': dqn_prediction_error, 'average_q_values': average_q_values }\n",
        "#             results_df = pd.Dataframe(results_dict)\n",
        "\n",
        "#             #visualise the results with sns\n",
        "#             # create 3 subplots\n",
        "#             visualise_results(results_df, step)"
      ],
      "metadata": {
        "id": "_ozhmEvBvr6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class ExperienceReplayBuffer:\n",
        "#     \"\"\"\n",
        "#     max_size: maximum size of the replay buffer\n",
        "#     env_name: name of the environment\n",
        "#     parallel_game_unrolls: number of parallel games to play ( we run multiple independent copies of same env in parallel with gym.vactor.VectorEnv)\n",
        "#     observation_preprocessing_function: function that preprocesses the observation\n",
        "#     \"\"\"\n",
        "#     def __init__(self, max_size:int, env_name: str, parallel_game_unrolls: int, observation_preprocessing_function: callable, unroll_steps: int):\n",
        "#         self.max_size = max_size\n",
        "#         self.env_name = env_name\n",
        "#         self.parallel_game_unrolls = parallel_game_unrolls\n",
        "#         self.unroll_steps = unroll_steps\n",
        "#         self.observation_preprocessing_function = observation_preprocessing_function\n",
        "#         self.envs = gym.vector.make(env_name, self.parallel_game_unrolls)\n",
        "#         self.num_possible_actions = self.envs.single_action_space.n\n",
        "#         self.current_states, _ = self.envs.reset()\n",
        "#         self.data = [] # fill this up step by step\n",
        "\n",
        "#     def fill_with_samples(self, dqn_network, epsilon: float):\n",
        "#         states_list = [] # length of this list is unroll_steps\n",
        "#         actions_list = []\n",
        "#         rewards_list = []\n",
        "#         subsequent_states_list = []\n",
        "#         terminateds_list = [] # each timestep is a batch\n",
        "\n",
        "#         # in each of these steps we take 1 step in each of all our parallel_game_unrolls environments\n",
        "#         # adds new samples into ERP\n",
        "#         for i in range(self.unroll_steps):\n",
        "\n",
        "#             actions = self.sample_epsilon_greedy_action(dqn_network, epsilon)\n",
        "#             print(\"actions: \", actions.shape)\n",
        "#             next_states, rewards, terminateds, _, _ = self.envs.step(actions)\n",
        "\n",
        "#             # put state, action, reward, next observation into erp\n",
        "#             states_list.append(self.current_states)\n",
        "#             actions_list.append(actions)\n",
        "#             rewards_list.append(rewards)\n",
        "#             subsequent_states_list.append(next_states)\n",
        "#             terminateds_list.append(terminateds)\n",
        "\n",
        "#             self.current_states = next_states\n",
        "\n",
        "#         # create a generator that yields a sample for each of the timesteps\n",
        "#         def data_generator():\n",
        "\n",
        "#             # walk through all timesteps created and for each timestep walk through all respective game to create 1 samle for each of these\n",
        "#             for states_batch, actions_batch, rewards_batch, subsequent_states_batch, terminateds_batch in zip(states_list, actions_list, rewards_list, subsequent_states_list, terminateds_list):\n",
        "#               print(\"shapes batches: s, a, r, ss, t \", states_batch.shape, actions_batch.shape, rewards_batch.shape, subsequent_states_batch.shape, terminateds_batch.shape)\n",
        "#               for game_idx in range(self.parallel_game_unrolls):\n",
        "#                 state = states_batch[game_idx, :, :, :]\n",
        "#                 action = actions_batch[game_idx]\n",
        "#                 reward = rewards_batch[game_idx]\n",
        "#                 subsequent_state = subsequent_states_batch[game_idx, :, :, :]\n",
        "#                 terminated = terminateds_batch[game_idx]\n",
        "#                 # state = states_batch(game_idx, _, _, _)# batch, h, w, c\n",
        "#                 # #print(\"inner state: \", state.shape, states_batch[game_idx, :, :, :].shape)\n",
        "#                 # action = actions_batch(game_idx)\n",
        "#                 # #print(\"inner action: \", action.shape, actions_batch[game_idx].shape)\n",
        "#                 # reward = rewards_batch(game_idx)\n",
        "#                 # subsequent_state = subsequent_states_batch(game_idx)#[game_idx, :, :, :]\n",
        "#                 # #print(\"inner state: \", subsequent_state.shape, subsequent_states_batch[game_idx, :, :, :].shape)\n",
        "#                 # terminated = terminateds_batch(game_idx)\n",
        "#                 print(\"shapes: s, a, r, ss, t \", state.shape, action.shape, reward.shape, subsequent_state.shape, terminated.shape)\n",
        "#                 yield (state, action, reward, subsequent_state, terminated)\n",
        "\n",
        "#         print(\"self.envs.action_space.shape: \", self.envs.action_space.shape)\n",
        "#         dataset_tensor_specs = (tf.TensorSpec(shape=(210, 160, 3), dtype=tf.uint8),\n",
        "#                                 tf.TensorSpec(shape=self.envs.action_space.shape, dtype=tf.int32),\n",
        "#                                 tf.TensorSpec(shape = (), dtype=tf.float32),\n",
        "#                                 tf.TensorSpec(shape=(210, 160, 3), dtype=tf.uint8),\n",
        "#                                 tf.TensorSpec(shape = (), dtype=tf.bool))\n",
        "\n",
        "#         # create for each step, we create sperate dataset\n",
        "#         new_samples_dataset = tf.data.Dataset.from_generator(data_generator, output_signature=dataset_tensor_specs)\n",
        "#         print(\"self.observation_preprocessing_function(state): \", states_list[0].shape, (self.observation_preprocessing_function(states_list[0])).shape)\n",
        "#         # Apply preprocessing first\n",
        "#         new_samples_dataset = new_samples_dataset.map(lambda state, action, reward, subsequent_state, terminated:(self.observation_preprocessing_function(state), action, reward, self.observation_preprocessing_function(subsequent_state), terminated)) #TODO\n",
        "#         # shuffle on each iteration\n",
        "#         new_samples_dataset = new_samples_dataset.cache()\n",
        "#         new_sample_dataset = new_samples_dataset.shuffle(buffer_size=self.parallel_game_unrolls*self.unroll_steps, reshuffle_each_iteration=True)\n",
        "\n",
        "#         # make sure cache is applied!\n",
        "#         count = 0\n",
        "#         for elem in new_samples_dataset:\n",
        "#           if count < 2:\n",
        "#             print(elem)\n",
        "#           continue\n",
        "#         # print(\"new_samples_dataset secured\")\n",
        "\n",
        "#         self.data.append(new_samples_dataset)\n",
        "#         datapoints_in_data = len(self.data) * self.parallel_game_unrolls * self.unroll_steps # last two calculate how many s, a, s', t tuples in one element of datalist\n",
        "#         if datapoints_in_data > self.max_size:\n",
        "#           self.data.pop(0)\n",
        "\n",
        "\n",
        "#     def create_dataset(self):\n",
        "#         # creates a tf.data.Dataset object from the ERP\n",
        "#         erp_dataset = tf.data.Dataset.sample_from_datasets(self.data, weights = [1/float(len(self.data)) for _ in self.data], stop_on_empty_dataset = False) # choose from datasets? #TODO\n",
        "#         return erp_dataset\n",
        "\n",
        "#     def sample_epsilon_greedy_action(self, dqn_network, epsilon: float):\n",
        "#         # samples actions from dqn_network using epsilon greedy fashion\n",
        "#         observations = self.observation_preprocessing_function(self.current_states)\n",
        "#         #print(\"self.current_states: \", (self.current_states).shape)\n",
        "#         q_values = dqn_network(observations) # tensor of type tf.float32, shape (parallel_game_unrolls, num_actions)\n",
        "#         print(\"q_values: \", q_values.shape)\n",
        "#         greedy_actions = tf.argmax(q_values, axis=1) # tensor of type tf.int64, shape (parallel_game_unrolls, )\n",
        "#         print(\"greedy_actions: \", greedy_actions.shape)\n",
        "#         random_actions = tf.random.uniform(shape=(self.parallel_game_unrolls,), minval=0, maxval=self.num_possible_actions, dtype=tf.int64) # tensor of type tf.int64, shape (parallel_game_unrolls, )\n",
        "\n",
        "#         # sample random actions with probability epsilon\n",
        "#         # prob 1-epsilon to take greedy action -> (p > epsilon) is true\n",
        "#         # prob epsilon to take random action\n",
        "#         greedy_actions_mask = tf.random.uniform(shape=(self.parallel_game_unrolls,), minval=0, maxval=1, dtype=tf.float32) > epsilon # tensor of type tf.bool, shape (parallel_game_unrolls, )\n",
        "#         actions = tf.where(greedy_actions_mask, greedy_actions, random_actions).numpy() # tensor of type tf.int64, shape (parallel_game_unrolls, )\n",
        "#         return actions"
      ],
      "metadata": {
        "id": "uBdboMUkCv8o"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_dqn(train_dqn_network, target_network, dataset, optimizer, gamma: float, num_training_steps:int, batch_size: int):\n",
        "\n",
        "#     dataset = dataset.batch(batch_size).prefetch(4)\n",
        "\n",
        "#     @tf.function\n",
        "#     def training_step(q_targets, observations, actions):\n",
        "#         with tf.GradientTape() as tape:\n",
        "#             q_predictions_all_actions = train_dqn_network(observations) #shape of q_predictions is (batch_size, num_actions)\n",
        "#             q_predictions = tf.gather(q_predictions_all_actions, actions, batch_dims=1) # find q-values at right indices from q_predictions_all_actions\n",
        "#             loss = tf.reduce_mean(tf.square(q_predictions - q_targets))\n",
        "#         gradients = tape.gradient(loss, train_dqn_network.trainable_variables)\n",
        "#         optimizer.apply_gradients(zip(gradients, train_dqn_network.trainable_variables))\n",
        "#         return loss.numpy()\n",
        "\n",
        "#     losses = []\n",
        "#     q_values = []\n",
        "#     #some preparations\n",
        "#     for i, state_transition in enumerate(dataset):\n",
        "#         state, action, reward, subsequent_state, terminated = state_transition\n",
        "#         #calculate q_targets\n",
        "#         q_vals =  target_network(subsequent_state) #TODO: check if this is correct\n",
        "#         q_values.append(q_vals.numpy())\n",
        "#         max_q_values = tf.reduce_max(q_vals, axis=1)\n",
        "#         use_subsequent_state = tf.where(terminated, tf.zeros_like(max_q_values, dtype=tf.float32), tf.ones_like(max_q_values, dtype=tf.float32))\n",
        "#         # calculate q_targets\n",
        "#         q_target = reward + (gamma*max_q_values*use_subsequent_state)\n",
        "#         #train on data\n",
        "#         loss = training_step(q_target, observations=state, actions=action).numpy()\n",
        "#         losses.append(loss)\n",
        "#         if i >= num_training_steps:\n",
        "#             break\n",
        "\n",
        "#     return np.mean(losses), np.mean(q_values)"
      ],
      "metadata": {
        "id": "PS0SPFmSvcyH"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}