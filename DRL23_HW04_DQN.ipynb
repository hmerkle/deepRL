{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP54+X3mHUeaKFAi6EbjUqH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hmerkle/deepRL/blob/main/DRL23_HW04_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- inner loop: create steps that you put in replay buffer\n",
        "- train q-network: set y_j"
      ],
      "metadata": {
        "id": "BUdOzcCMDFhZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pip install and imports"
      ],
      "metadata": {
        "id": "1eYbnY0HFugX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install \"gymnasium[accept-rom-license, atari]\""
      ],
      "metadata": {
        "id": "tMdXfYgSE1SY"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade ipykernel"
      ],
      "metadata": {
        "id": "g-e4yY6MFek-"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "7Fo9NsQsEVFs"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# code"
      ],
      "metadata": {
        "id": "Yi7mJZvHFyg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperienceReplayBuffer:\n",
        "    \"\"\"\n",
        "    max_size: maximum size of the replay buffer\n",
        "    env_name: name of the environment\n",
        "    parallel_game_unrolls: number of parallel games to play ( we run multiple independent copies of same env in parallel with gym.vactor.VectorEnv)\n",
        "    observation_preprocessing_function: function that preprocesses the observation\n",
        "    \"\"\"\n",
        "    def __init__(self, max_size:int, env_name: str, parallel_game_unrolls: int, observation_preprocessing_function: callable, unroll_steps: int):\n",
        "        self.max_size = max_size\n",
        "        self.env_name = env_name\n",
        "        self.parallel_game_unrolls = parallel_game_unrolls\n",
        "        self.unroll_steps = unroll_steps\n",
        "        self.observation_preprocessing_function = observation_preprocessing_function\n",
        "        self.envs = gym.vector.make(env_name, self.parallel_game_unrolls)\n",
        "        self.num_possible_actions = self.envs.single_action_space.n\n",
        "        self.current_states, _ = self.envs.reset()\n",
        "        self.data = [] # fill this up step by step\n",
        "\n",
        "    def fill_with_samples(self, dqn_network, epsilon: float):\n",
        "        states_list = [] # length of this list is unroll_steps\n",
        "        actions_list = []\n",
        "        rewards_list = []\n",
        "        subsequent_states_list = []\n",
        "        terminateds_list = [] # each timestep is a batch\n",
        "\n",
        "        # in each of these steps we take 1 step in each of all our parallel_game_unrolls environments\n",
        "        # adds new samples into ERP\n",
        "        for i in range(self.unroll_steps):\n",
        "\n",
        "            actions = self.sample_epsilon_greedy_action(dqn_network, epsilon)\n",
        "            print(\"actions: \", actions.shape)\n",
        "            next_states, rewards, terminateds, _, _ = self.envs.step(actions)\n",
        "\n",
        "            # put state, action, reward, next observation into erp\n",
        "            observations = self.observation_preprocessing_function(self.current_states)\n",
        "            states_list.append(self.current_states)\n",
        "            #print(\"states_list: \", states_list[0])\n",
        "            actions_list.append(actions)\n",
        "            rewards_list.append(rewards)\n",
        "            subsequent_states_list.append(next_states)\n",
        "            terminateds_list.append(terminateds)\n",
        "\n",
        "            self.current_states = next_states\n",
        "\n",
        "        # create a generator that yields a sample for each of the timesteps\n",
        "        def data_generator():\n",
        "\n",
        "            # walk through all timesteps created and for each timestep walk through all respective game to create 1 samle for each of these\n",
        "            for states_batch, actions_batch, rewards_batch, subsequent_states_batch, terminateds_batch in zip(states_list, actions_list, rewards_list, subsequent_states_list, terminateds_list):\n",
        "              print(\"shapes batches: s, a, r, ss, t \", states_batch.shape, actions_batch.shape, rewards_batch.shape, subsequent_states_batch.shape, terminateds_batch.shape)\n",
        "              for game_idx in range(self.parallel_game_unrolls):\n",
        "                state = states_batch[game_idx, :, :, :]\n",
        "                action = actions_batch[game_idx]\n",
        "                reward = rewards_batch[game_idx]\n",
        "                subsequent_state = subsequent_states_batch[game_idx, :, :, :]\n",
        "                terminated = terminateds_batch[game_idx]\n",
        "                # state = states_batch(game_idx, _, _, _)# batch, h, w, c\n",
        "                # #print(\"inner state: \", state.shape, states_batch[game_idx, :, :, :].shape)\n",
        "                # action = actions_batch(game_idx)\n",
        "                # #print(\"inner action: \", action.shape, actions_batch[game_idx].shape)\n",
        "                # reward = rewards_batch(game_idx)\n",
        "                # subsequent_state = subsequent_states_batch(game_idx)#[game_idx, :, :, :]\n",
        "                # #print(\"inner state: \", subsequent_state.shape, subsequent_states_batch[game_idx, :, :, :].shape)\n",
        "                # terminated = terminateds_batch(game_idx)\n",
        "                print(\"shapes: s, a, r, ss, t \", state.shape, action.shape, reward.shape, subsequent_state.shape, terminated.shape)\n",
        "                yield (state, action, reward, subsequent_state, terminated)\n",
        "\n",
        "        print(\"self.envs.action_space.shape: \", self.envs.action_space.shape)\n",
        "        dataset_tensor_specs = (tf.TensorSpec(shape=(210, 160, 3), dtype=tf.uint8),\n",
        "                                tf.TensorSpec(shape=self.envs.action_space.shape, dtype=tf.int32),\n",
        "                                tf.TensorSpec(shape = (), dtype=tf.float32),\n",
        "                                tf.TensorSpec(shape=(210, 160, 3), dtype=tf.uint8),\n",
        "                                tf.TensorSpec(shape = (), dtype=tf.bool))\n",
        "\n",
        "        # create for each step, we create sperate dataset\n",
        "        new_samples_dataset = tf.data.Dataset.from_generator(data_generator, output_signature=dataset_tensor_specs)\n",
        "        print(\"self.observation_preprocessing_function(state): \", states_list[0].shape, (self.observation_preprocessing_function(states_list[0])).shape)\n",
        "        # Apply preprocessing first\n",
        "        new_samples_dataset = new_samples_dataset.map(lambda state, action, reward, subsequent_state, terminated:(self.observation_preprocessing_function(state), action, reward, self.observation_preprocessing_function(subsequent_state), terminated)) #TODO\n",
        "        # shuffle on each iteration\n",
        "        new_samples_dataset = new_samples_dataset.cache()#.shuffle(buffer_size=self.parallel_game_unrolls*self.unroll_steps, reshuffle_each_iteration=True)\n",
        "\n",
        "        # make sure cache is applied!\n",
        "        count = 0\n",
        "        for elem in new_samples_dataset:\n",
        "          if count < 2:\n",
        "            print(elem)\n",
        "          continue\n",
        "        # print(\"new_samples_dataset secured\")\n",
        "\n",
        "        self.data.append(new_samples_dataset)\n",
        "        datapoints_in_data = len(self.data) * self.parallel_game_unrolls * self.unroll_steps # last two calculate how many s, a, s', t tuples in one element of datalist\n",
        "        if datapoints_in_data > self.max_size:\n",
        "          self.data.pop(0)\n",
        "\n",
        "\n",
        "    def create_dataset(self):\n",
        "        # creates a tf.data.Dataset object from the ERP\n",
        "        erp_dataset = tf.data.Dataset.sample_from_datasets(self.data, weights = [1/float(len(self.data)) for _ in self.data], stop_on_empty_dataset = False) # choose from datasets? #TODO\n",
        "        return erp_dataset\n",
        "\n",
        "    def sample_epsilon_greedy_action(self, dqn_network, epsilon: float):\n",
        "        # samples actions from dqn_network using epsilon greedy fashion\n",
        "        observations = self.observation_preprocessing_function(self.current_states)\n",
        "        #print(\"self.current_states: \", (self.current_states).shape)\n",
        "        q_values = dqn_network(observations) # tensor of type tf.float32, shape (parallel_game_unrolls, num_actions)\n",
        "        print(\"q_values: \", q_values.shape)\n",
        "        greedy_actions = tf.argmax(q_values, axis=1) # tensor of type tf.int64, shape (parallel_game_unrolls, )\n",
        "        print(\"greedy_actions: \", greedy_actions.shape)\n",
        "        random_actions = tf.random.uniform(shape=(self.parallel_game_unrolls,), minval=0, maxval=self.num_possible_actions, dtype=tf.int64) # tensor of type tf.int64, shape (parallel_game_unrolls, )\n",
        "\n",
        "        # sample random actions with probability epsilon\n",
        "        # prob 1-epsilon to take greedy action -> (p > epsilon) is true\n",
        "        # prob epsilon to take random action\n",
        "        greedy_actions_mask = tf.random.uniform(shape=(self.parallel_game_unrolls,), minval=0, maxval=1, dtype=tf.float32) > epsilon # tensor of type tf.bool, shape (parallel_game_unrolls, )\n",
        "        actions = tf.where(greedy_actions_mask, greedy_actions, random_actions).numpy() # tensor of type tf.int64, shape (parallel_game_unrolls, )\n",
        "        return actions"
      ],
      "metadata": {
        "id": "uBdboMUkCv8o"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def observation_preprocessing_function(observation):\n",
        "    # (210, 160, 3) to (84, 84, 3)\n",
        "    #converts an observation to a tensor of type tf.float32, size(84,84)\n",
        "    observation = tf.image.resize(observation, size=(84,84))#, resize=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    observation = tf.cast(observation, dtype=tf.float32) /128.0 - 1.0 # values between -1 and 1\n",
        "\n",
        "    return observation"
      ],
      "metadata": {
        "id": "TouJpOSjvZRb"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dqn_network(num_actions: int):\n",
        "    #create an input for tf functional model api\n",
        "    input_layer = tf.keras.Input(shape=(84, 84, 3), dtype= tf.float32)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu')(input_layer)\n",
        "    x = tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu', padding='same')(x) + x #residual connection\n",
        "    x = tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu', padding='same')(x) + x #residual connection\n",
        "    x = tf.keras.layers.MaxPool2D(pool_size=2)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu')(x)\n",
        "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')(x) + x #residual connection\n",
        "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')(x) + x #residual connection\n",
        "    x = tf.keras.layers.MaxPool2D(pool_size=2)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu')(x)\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x) + x #residual connection\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x) + x #residual connection\n",
        "    x = tf.keras.layers.MaxPool2D(pool_size=2)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu')(x)\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x) + x #residual connection\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x) + x #residual connection\n",
        "    x = tf.keras.layers.GlobalAvgPool2D()(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(units=64, activation='relu')(x) + x #residual connection\n",
        "    x = tf.keras.layers.Dense(units=num_actions, activation='linear')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=x)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "TLSwzH9jvapP"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_dqn(train_dqn_network, target_network, dataset, optimizer, gamma: float, num_training_steps:int, batch_size: int):\n",
        "\n",
        "#     dataset = dataset.batch(batch_size).prefetch(4)\n",
        "\n",
        "#     @tf.function\n",
        "#     def training_step(q_targets, observations, actions):\n",
        "#         with tf.GradientTape() as tape:\n",
        "#             q_predictions_all_actions = train_dqn_network(observations) #shape of q_predictions is (batch_size, num_actions)\n",
        "#             q_predictions = tf.gather(q_predictions_all_actions, actions, batch_dims=1) # find q-values at right indices from q_predictions_all_actions\n",
        "#             loss = tf.reduce_mean(tf.square(q_predictions - q_targets))\n",
        "#         gradients = tape.gradient(loss, train_dqn_network.trainable_variables)\n",
        "#         optimizer.apply_gradients(zip(gradients, train_dqn_network.trainable_variables))\n",
        "#         return loss.numpy()\n",
        "\n",
        "#     losses = []\n",
        "#     q_values = []\n",
        "#     #some preparations\n",
        "#     for i, state_transition in enumerate(dataset):\n",
        "#         state, action, reward, subsequent_state, terminated = state_transition\n",
        "#         #calculate q_targets\n",
        "#         q_vals =  target_network(subsequent_state) #TODO: check if this is correct\n",
        "#         q_values.append(q_vals.numpy())\n",
        "#         max_q_values = tf.reduce_max(q_vals, axis=1)\n",
        "#         use_subsequent_state = tf.where(terminated, tf.zeros_like(max_q_values, dtype=tf.float32), tf.ones_like(max_q_values, dtype=tf.float32))\n",
        "#         # calculate q_targets\n",
        "#         q_target = reward + (gamma*max_q_values*use_subsequent_state)\n",
        "#         #train on data\n",
        "#         loss = training_step(q_target, observations=state, actions=action).numpy()\n",
        "#         losses.append(loss)\n",
        "#         if i >= num_training_steps:\n",
        "#             break\n",
        "\n",
        "#     return np.mean(losses), np.mean(q_values)"
      ],
      "metadata": {
        "id": "PS0SPFmSvcyH"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dqn(train_network, target_network, dataset, optimizer, gamma: float, num_training_steps:int, batch_size: int):\n",
        "\n",
        "    dataset = dataset.batch(batch_size).prefetch(4)\n",
        "\n",
        "    @tf.function\n",
        "    def training_step(q_targets, observations, actions):\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_predictions_all_actions = train_network(observations) #shape of q_predictions is (batch_size, num_actions)\n",
        "            q_predictions = tf.gather(q_predictions_all_actions, actions, batch_dims=1) # find q-values at right indices from q_predictions_all_actions\n",
        "            loss = tf.reduce_mean(tf.square(q_predictions - q_targets))\n",
        "        gradients = tape.gradient(loss, train_network.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, train_network.trainable_variables))\n",
        "        return loss.numpy()\n",
        "\n",
        "    losses = []\n",
        "    q_values = []\n",
        "    #some preparations\n",
        "    for i, state_transition in enumerate(dataset):\n",
        "        state, action, reward, subsequent_state, terminated = state_transition\n",
        "        #calculate q_targets\n",
        "        q_vals =  target_network(subsequent_state) #TODO: check if this is correct\n",
        "        q_values.append(q_vals.numpy())\n",
        "        max_q_values = tf.reduce_max(q_vals, axis=1)\n",
        "        use_subsequent_state = tf.where(terminated, tf.zeros_like(max_q_values, dtype=tf.float32), tf.ones_like(max_q_values, dtype=tf.float32))\n",
        "        # calculate q_targets\n",
        "        q_target = reward + (gamma*max_q_values*use_subsequent_state)\n",
        "        #train on data\n",
        "        loss = training_step(q_target, observations=state, actions=action).numpy()\n",
        "        losses.append(loss)\n",
        "        if i >= num_training_steps:\n",
        "            break\n",
        "\n",
        "    return np.mean(losses), np.mean(q_values)"
      ],
      "metadata": {
        "id": "JoZpoCoqHKMZ"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_dqn_network(test_dqn_network, env_name: str, num_parallel_game_tests:int, gamma: float, preprocessing_function: callable, test_epsilon: float=0.05):\n",
        "    envs = gym.vector.make(env_name, num_envs=num_parallel_game_tests)\n",
        "    num_possible_actions = envs.single_action_space.n\n",
        "    states,_ = envs.reset()\n",
        "\n",
        "    #episodes finished is a numpy vector of shape (num_parallel_tests), filled with Booleans, starting with all False\n",
        "    episodes_finished = np.zeros(num_parallel_game_tests, dtype=bool)\n",
        "    returns = np.zeros(num_parallel_game_tests)\n",
        "\n",
        "    done=False\n",
        "    timestep = 0\n",
        "    while not done:\n",
        "        states = preprocessing_function(states)\n",
        "        q_values = test_dqn_network(states)\n",
        "        greedy_actions = tf.argmax(q_values, axis=1) # tensor of type tf.int64, shape (num_parallel_game_tests,)\n",
        "        random_actions = tf.random.uniform(shape=(num_parallel_game_tests,), minval=0, maxval=num_possible_actions, dtype=tf.int64)\n",
        "        epsilon_sampling = tf.random.uniform(shape=(num_parallel_game_tests,), minval=0, maxval=1, dtype=tf.float32) > test_epsilon\n",
        "        actions = tf.where(epsilon_sampling, greedy_actions, random_actions).numpy() # type numpy.ndarray, shape (num_parallel_game_tests,)\n",
        "        states, rewards, terminateds, _, _ = envs.step(actions)\n",
        "\n",
        "        #compute pointwise or between episodes_finished and terminateds\n",
        "        episodes_finished = np.logical_or(episodes_finished, terminateds) # if an episode is finished, it stays finished\n",
        "        returns += ((gamma**timestep)*rewards)*(np.logical_not(episodes_finished).astype(np.float32)) # only add rewards if episode is not finished -> multiply with 0 if episode is finished\n",
        "        timestep += 1\n",
        "        #done if all episodes are finished\n",
        "        done = np.all(episodes_finished)\n",
        "        test_steps += 1\n",
        "        if test_steps%100 == 0:\n",
        "            print(\"Test steps: \", test_steps, np.sum(episodes_finished)/num_parallel_game_tests, terminateds.shape, episodes_finished.shape)\n",
        "\n",
        "    return np.mean(returns)"
      ],
      "metadata": {
        "id": "wu6_eWumvf5P"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Take a fraction of the source network weights and copy weights to target network weights\n",
        "# def polyak_averaging_weigths(source_network, target_network, polyak_averaging_factor: float):\n",
        "#     source_network_weights = source_network.get_weights()\n",
        "#     target_network_weights = target_network.get_weights()\n",
        "#     averaged_weights = []\n",
        "\n",
        "#     for source_network_weight, target_network_weight in zip(source_network_weights, target_network_weights):\n",
        "#         fraction_kept_weights = polyak_averaging_factor*target_network_weight # weight that are kept from previous iteration\n",
        "#         fraction_updated_weights = (1 - polyak_averaging_factor) * source_network_weight\n",
        "#         average_weight = fraction_kept_weights + fraction_updated_weights\n",
        "#         averaged_weights.append(average_weight)\n",
        "#     target_network.set_weights(averaged_weights)"
      ],
      "metadata": {
        "id": "CnTwig50vjro"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a fraction of the source network weights and copy weights to target network weights\n",
        "def polyak_averaging_weigths(source_network, target_network, polyak_averaging_factor: float):\n",
        "    source_network_weights = source_network.get_weights()\n",
        "    target_network_weights = target_network.get_weights()\n",
        "    averaged_weights = []\n",
        "\n",
        "    for source_network_weight, target_network_weight in zip(source_network_weights, target_network_weights):\n",
        "        fraction_kept_weights = polyak_averaging_factor*source_network_weight # weight that are kept from previous iteration\n",
        "        fraction_updated_weights = (1 - polyak_averaging_factor) * target_network_weight\n",
        "        average_weight = fraction_kept_weights + fraction_updated_weights\n",
        "        averaged_weights.append(average_weight)\n",
        "    target_network.set_weights(averaged_weights)"
      ],
      "metadata": {
        "id": "ajjM4w7-MkJ7"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualise_results(result_df, step):\n",
        "    # create a figure with 3 subplots\n",
        "    fig, axis = plt.subplots(3, 1)\n",
        "    # include row idxs explicitely in results_df\n",
        "    result_df['step'] = result_df.index\n",
        "    # plot average returns\n",
        "    sns.lineplot(x = \"step\", y = \"average_returns\", data = result_df, ax = axis[0])\n",
        "    # plot average losses\n",
        "    sns.lineplot(x = \"step\", y = \"average_losses\", data = result_df, ax = axis[1])\n",
        "    # plot average q-values\n",
        "    sns.lineplot(x = \"step\", y = \"average_q_values\", data = result_df, ax = axis[2])\n",
        "    # # save the figure\n",
        "    # plt.savefig(\"average_q_values.png\")\n",
        "    # # create a timestring from the timestamp\n",
        "    # timestring = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    # # save the figure\n",
        "    # plt.savefig(f'./results/{timestring}_results_step{step}.png')\n",
        "    # plt.close(fig)"
      ],
      "metadata": {
        "id": "J6grNi7Svmqp"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dqn():\n",
        "    ENVIRONMENT_NAME = 'ALE/Breakout-v5'\n",
        "    NUM_ACTIONS = gym.make(ENVIRONMENT_NAME).action_space.n\n",
        "    ERP_SIZE= 1000#100000 # experience replay buffer size\n",
        "    PARALLEL_GAME_UNROLLS = 1#24#128 # number of parallel games to play\n",
        "    UNROLL_STEPS = 1 # number of steps to unroll each game\n",
        "    EPSILON = 0.2\n",
        "    GAMMA = 0.995\n",
        "    NUM_TRAINING_STEPS_PER_ITERATION = 16\n",
        "    TRAIN_BATCH_SIZE = 128#512\n",
        "    NUM_TRAINING_ITERS = 3#50000\n",
        "    TEST_EVERY_N_STEPS = 50\n",
        "    TEST_NUM_PARALLEL_ENVS = 1#24#128\n",
        "    PREFILL_STEPS = 1#24#100\n",
        "    POLYAK_AVERAGING_FACTOR = 0.99\n",
        "\n",
        "    # container containing all  s, a, r, s', t transitions\n",
        "    # With deep Q-networks, we often utilize this technique called experience replay during training. With experience replay, we store\n",
        "    # the agent's experiences at each time step in a data set called the replay memory\n",
        "    erp = ExperienceReplayBuffer(max_size=ERP_SIZE,\n",
        "                                env_name=ENVIRONMENT_NAME,\n",
        "                                parallel_game_unrolls=PARALLEL_GAME_UNROLLS,\n",
        "                                observation_preprocessing_function=observation_preprocessing_function,\n",
        "                                unroll_steps=UNROLL_STEPS)\n",
        "\n",
        "    #input is some image from game and outputs atari output\n",
        "    # dqn that is trained\n",
        "    dqn_agent = create_dqn_network(num_actions=NUM_ACTIONS)\n",
        "    #this is the target network, used to calculate the q_estimation targets\n",
        "    target_network = create_dqn_network(num_actions=NUM_ACTIONS)\n",
        "\n",
        "    #dqn_agent(tf.random.uniform(shape=(1, 84, 84, 3)))\n",
        "    #dqn_agent.summary()\n",
        "\n",
        "    #copy over the weights from the dqn_agent to the target_network via polyak averaging with factor 0.8\n",
        "    polyak_averaging_weigths(source_network=dqn_agent, target_network=target_network, polyak_averaging_factor=0.0)#0.8)\n",
        "\n",
        "    dqn_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "    #prefill the replay buffer -> less  bias\n",
        "    prefill_exploration_epsilon = 1.0\n",
        "    for prefill_step in range(PREFILL_STEPS):\n",
        "        erp.fill_with_samples(dqn_agent, prefill_exploration_epsilon)\n",
        "\n",
        "    #test the agent\n",
        "    return_tracker = []\n",
        "    dqn_prediction_error = []\n",
        "    average_q_values = []\n",
        "\n",
        "    for step in range(NUM_TRAINING_ITERS):\n",
        "        print(\"Training iteration: \", step)\n",
        "        #step 1: put some s, a, r, s' transitions into the replay buffer\n",
        "        erp.fill_with_samples(dqn_agent, EPSILON)\n",
        "        dataset = erp.create_dataset()\n",
        "\n",
        "        #step 2: train on some samples from the replay buffer\n",
        "        average_loss, average_q_value = train_dqn(dqn_agent, target_network, dataset, optimizer=dqn_optimizer, gamma=GAMMA, num_training_steps=NUM_TRAINING_STEPS_PER_ITERATION, batch_size=TRAIN_BATCH_SIZE)\n",
        "        #update the target network via polyak averaging\n",
        "        erp.polyak_averaging_weights(source_network=dqn_agent, target_network=target_network, polyak_averaging_factor=POLYAK_AVERAGING_FACTOR)\n",
        "\n",
        "        #step 3: test the agent\n",
        "        if step % TEST_EVERY_N_STEPS == 0:  #test every N steps\n",
        "            average_returns = test_dqn_network(dqn_agent, ENVIRONMENT_NAME, num_parallel_tests=TEST_NUM_PARALLEL_ENVS, gamma=GAMMA )\n",
        "            return_tracker.append(average_returns)\n",
        "            dqn_prediction_error.append(average_loss)\n",
        "            average_q_values.append(average_q_value)\n",
        "            #print average loss, average returns, average q_values\n",
        "            print(f'average return: {average_returns}, TESTING: average loss: {average_loss}, average q_value-estimate: {average_q_value}')\n",
        "\n",
        "            #put all the result lists into a dataframe (by transforming them into a dict first)\n",
        "            results_dict = {'average_return': return_tracker, 'average_loss': dqn_prediction_error, 'average_q_values': average_q_values }\n",
        "            results_df = pd.Dataframe(results_dict)\n",
        "\n",
        "            #visualise the results with sns\n",
        "            # create 3 subplots\n",
        "            visualise_results(results_df, step)"
      ],
      "metadata": {
        "id": "ZNhl0w1MBm7M"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    dqn()"
      ],
      "metadata": {
        "id": "a7uwilXTLrYI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        },
        "outputId": "952ab88d-f733-4748-e2f9-47c5b440d0bd"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q_values:  (1, 4)\n",
            "greedy_actions:  (1,)\n",
            "actions:  (1,)\n",
            "self.envs.action_space.shape:  (1,)\n",
            "self.observation_preprocessing_function(state):  (1, 210, 160, 3) (1, 84, 84, 3)\n",
            "shapes batches: s, a, r, ss, t  (1, 210, 160, 3) (1,) (1,) (1, 210, 160, 3) (1,)\n",
            "shapes: s, a, r, ss, t  (210, 160, 3) () () (210, 160, 3) ()\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-148-bc5b70255259>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-147-c24be49f6ba2>\u001b[0m in \u001b[0;36mdqn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprefill_exploration_epsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mprefill_step\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPREFILL_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0merp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_with_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdqn_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefill_exploration_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m#test the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-138-a94ee0778c69>\u001b[0m in \u001b[0;36mfill_with_samples\u001b[0;34m(self, dqn_network, epsilon)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# make sure cache is applied!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_samples_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    795\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    778\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    781\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3014\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3015\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3016\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3017\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7260\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7261\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7262\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_5_device_/job:localhost/replica:0/task:0/device:CPU:0}} TypeError: `generator` yielded an element of shape () where an element of shape (1,) was expected.\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 267, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 235, in generator_py_func\n    raise TypeError(\n\nTypeError: `generator` yielded an element of shape () where an element of shape (1,) was expected.\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def dqn():\n",
        "#     ENVIRONMENT_NAME = 'ALE/Breakout-v5'\n",
        "#     NUM_ACTIONS = gym.make(ENVIRONMENT_NAME).action_space.n\n",
        "#     ERP_SIZE= 1000#100000 # experience replay buffer size\n",
        "#     PARALLEL_GAME_UNROLLS = 24#128 # number of parallel games to play\n",
        "#     UNROLL_STEPS = 4 # number of steps to unroll each game\n",
        "#     EPSILON = 0.2\n",
        "#     GAMMA = 0.995\n",
        "#     NUM_TRAINING_STEPS_PER_ITERATION = 16\n",
        "#     TRAIN_BATCH_SIZE = 128#512\n",
        "#     NUM_TRAINING_ITERS = 3#50000\n",
        "#     TEST_EVERY_N_STEPS = 50\n",
        "#     TEST_NUM_PARALLEL_ENVS = 24#128\n",
        "#     PREFILL_STEPS = 24#100\n",
        "#     POLYAK_AVERAGING_FACTOR = 0.99\n",
        "\n",
        "#     # container containing all  s, a, r, s', t transitions\n",
        "#     erp = ExperienceReplayBuffer(max_size=ERP_SIZE,\n",
        "#                                 env_name=ENVIRONMENT_NAME,\n",
        "#                                 parallel_game_unrolls=PARALLEL_GAME_UNROLLS,\n",
        "#                                 observation_preprocessing_function=observation_preprocessing_function,\n",
        "#                                 unroll_steps=UNROLL_STEPS)\n",
        "\n",
        "#     #input is some image from game and outputs atari output\n",
        "#     # dqn that is trained\n",
        "#     dqn_agent = create_dqn_network(num_actions=NUM_ACTIONS)\n",
        "#     #dqn_agent.summary()\n",
        "#     dqn_agent(tf.random.uniform(shape=(1, 84, 84, 3)))\n",
        "\n",
        "#     #this is the target network, used to calculate the q_estimation targets\n",
        "#     target_network = create_dqn_network(num_actions=NUM_ACTIONS)\n",
        "#     #copy over the weights from the dqn_agent to the target_network via polyak averaging with factor 0.8\n",
        "#     polyak_averaging_weigths(source_network=dqn_agent, target_network=target_network, polyak_averaging_factor=0.0)#0.8)\n",
        "\n",
        "#     dqn_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "#     #prefill the replay buffer -> less  bias\n",
        "#     prefill_exploration_epsilon = 1.0\n",
        "#     for prefill_step in range(PREFILL_STEPS):\n",
        "#         erp.fill_with_samples(dqn_agent, prefill_exploration_epsilon)\n",
        "\n",
        "#     #test the agent\n",
        "#     return_tracker = []\n",
        "#     dqn_prediction_error = []\n",
        "#     average_q_values = []\n",
        "\n",
        "#     for step in range(NUM_TRAINING_ITERS):\n",
        "#         print(\"Training iteration: \", step)\n",
        "#         #step 1: put some s, a, r, s' transitions into the replay buffer\n",
        "#         erp.fill_with_samples(dqn_agent, EPSILON)\n",
        "#         dataset = erp.create_dataset()\n",
        "\n",
        "#         #step 2: train on some samples from the replay buffer\n",
        "#         average_loss, average_q_value = train_dqn(dqn_agent, target_network, dataset, optimizer=dqn_optimizer, gamma=GAMMA, num_training_steps=NUM_TRAINING_STEPS_PER_ITERATION, batch_size=TRAIN_BATCH_SIZE)\n",
        "#         #update the target network via polyak averaging\n",
        "#         erp.polyak_averaging_weights(source_network=dqn_agent, target_network=target_network, polyak_averaging_factor=POLYAK_AVERAGING_FACTOR)\n",
        "\n",
        "#         #step 3: test the agent\n",
        "#         if step % TEST_EVERY_N_STEPS == 0:  #test every N steps\n",
        "#             average_returns = test_dqn_network(dqn_agent, ENVIRONMENT_NAME, num_parallel_tests=TEST_NUM_PARALLEL_ENVS, gamma=GAMMA )\n",
        "#             return_tracker.append(average_returns)\n",
        "#             dqn_prediction_error.append(average_loss)\n",
        "#             average_q_values.append(average_q_value)\n",
        "#             #print average loss, average returns, average q_values\n",
        "#             print(f'average return: {average_returns}, TESTING: average loss: {average_loss}, average q_value-estimate: {average_q_value}')\n",
        "\n",
        "#             #put all the result lists into a dataframe (by transforming them into a dict first)\n",
        "#             results_dict = {'average_return': return_tracker, 'average_loss': dqn_prediction_error, 'average_q_values': average_q_values }\n",
        "#             results_df = pd.Dataframe(results_dict)\n",
        "\n",
        "#             #visualise the results with sns\n",
        "#             # create 3 subplots\n",
        "#             visualise_results(results_df, step)"
      ],
      "metadata": {
        "id": "_ozhmEvBvr6y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}